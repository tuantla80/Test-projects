{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T11:46:53.884862Z",
     "iopub.status.busy": "2021-10-02T11:46:53.884526Z",
     "iopub.status.idle": "2021-10-02T11:46:56.102124Z",
     "shell.execute_reply": "2021-10-02T11:46:56.101344Z",
     "shell.execute_reply.started": "2021-10-02T11:46:53.884776Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/tez-lib/\")\n",
    "sys.path.append(\"../input/timmmaster/\")\n",
    "\n",
    "import os\n",
    "import tez            # a simple pytorch trainer\n",
    "from tez.callbacks import EarlyStopping\n",
    "import albumentations # Fast image augmentation library\n",
    "import pandas as pd\n",
    "import cv2            # OpenCV \n",
    "import numpy as np\n",
    "import timm           # (Unofficial) PyTorch Image Models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics, model_selection\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "class Args:\n",
    "    batch_size = 64\n",
    "    image_size = 256\n",
    "    epochs = 20 \n",
    "    \n",
    "path_input = r'../input/petfinder-pawpularity-score/'\n",
    "\n",
    "test_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(Args.image_size, Args.image_size, p=1),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T11:46:56.10443Z",
     "iopub.status.busy": "2021-10-02T11:46:56.10402Z",
     "iopub.status.idle": "2021-10-02T11:46:56.114921Z",
     "shell.execute_reply": "2021-10-02T11:46:56.113806Z",
     "shell.execute_reply.started": "2021-10-02T11:46:56.104393Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''To get data of each image'''\n",
    "    def __init__(self, \n",
    "                 image_paths, \n",
    "                 dense_features, \n",
    "                 targets, \n",
    "                 augmentations=None):\n",
    "        '''\n",
    "        :param dense_features: chosen feature columns\n",
    "            dense_features = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "                              'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n",
    "        '''\n",
    "        self.image_paths = image_paths\n",
    "        self.dense_features = dense_features\n",
    "        self.targets = targets\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        :param item: a row index of one image data\n",
    "        return: dic\n",
    "            Eg. dic = {'image': tensor data of the image,\n",
    "                       'features': a dense features corresponding to this image, \n",
    "                       'targets': Pawpularity of this image}        \n",
    "        '''        \n",
    "        image = cv2.imread(self.image_paths[item]) # Eg. image.shape = (960, 720, 3): W, H and C\n",
    "        \n",
    "        # as opencv loads in BGR format by default, we want to show it in RGB.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Converts an image from one color space to another\n",
    "                                                       # image.shape = (960, 720, 3): C here order RGB\n",
    "            \n",
    "        if self.augmentations is not None: # refer to train_aug and valid_aug\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32) # (0, 1, 2) -> (2, 0, 1)\n",
    "                                                                  #  W, H, C   -> C, W, H  \n",
    "        features = self.dense_features[item, :]  # item is row index. Ex. item=0 is the first image       \n",
    "        targets = self.targets[item]  # value of Pawpularity of an image\n",
    "        \n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float),  # np array to torch tensor\n",
    "            'features': torch.tensor(features, dtype=torch.float), \n",
    "            'targets': torch.tensor(targets,  dtype=torch.float)            \n",
    "        }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T11:46:56.116558Z",
     "iopub.status.busy": "2021-10-02T11:46:56.116262Z",
     "iopub.status.idle": "2021-10-02T11:46:56.131422Z",
     "shell.execute_reply": "2021-10-02T11:46:56.130772Z",
     "shell.execute_reply.started": "2021-10-02T11:46:56.116524Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(tez.Model):\n",
    "    '''\n",
    "    Note: in tez: Dataset === pytorch requires\n",
    "                  model class: nn.Module (pytorch) -> tez.Model\n",
    "    '''\n",
    "    def __init__(self, model_name):\n",
    "        '''\n",
    "        :param model_name: name of model for timm.create_model()\n",
    "            Eg1. model_name = 'tf_efficientnet_b0_ns'\n",
    "            Eg2. model_name = 'swin_large_patch4_window12_384'\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # model: EfficientNet\n",
    "        self.model = timm.create_model(model_name, \n",
    "                                       pretrained = False, \n",
    "                                       in_chans = 3)\n",
    "        \n",
    "        # Linear(in_features=1280, out_features=128, bias=True). print model to see the keywords\n",
    "        # Inside this model: (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
    "        # change it to:      (classifier): Linear(in_features=1280, out_features=128, bias=True)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier.in_features, 128) \n",
    "        \n",
    "        # Dropout(p=0.1, inplace=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Linear(in_features=140, out_features=1, bias=True)\n",
    "        self.out = nn.Linear(128 + 12, 1)  # 12 is from dense features -> 128+12=140\n",
    "        \n",
    "        self.step_scheduler_after = 'epoch'\n",
    "        \n",
    "        # Other ways\n",
    "        # self.dropout = nn.Dropout(0.1)\n",
    "        # self.dense1 = nn.Linear(140, 64)\n",
    "        # self.dense2 = nn.Linear(64, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, image, features, targets=None):\n",
    "        x = self.model(image)  # x.shape = torch.Size([1, 128]). Eg. [[-0.0453,....0.1661]]\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat([x, features], dim=1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        # Other ways\n",
    "        # x = self.model(image)\n",
    "        # x = self.dropout(x)\n",
    "        # x = torch.cat([x, features], dim=1)\n",
    "        # x = self.dense1(x)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.dense2(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = nn.MSELoss()(x, targets.view(-1, 1))\n",
    "            metrics = self.monitor_metrics(x, targets)\n",
    "            return x, loss, metrics\n",
    "        return x, 0, {}  \n",
    "        \n",
    "        \n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        \n",
    "        rmse = metrics.mean_squared_error(y_true=targets, \n",
    "                                          y_pred=outputs, \n",
    "                                          squared=False # True returns MSE, False returns RMSE\n",
    "                                         )\n",
    "        return {'rmse': rmse}\n",
    "    \n",
    "    \n",
    "    def fetch_scheduler(self):\n",
    "        '''\n",
    "        For Learning rate\n",
    "        Set the learning rate of each parameter group using a cosine annealing schedule\n",
    "        '''\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            T_0 = 10,\n",
    "            T_mult = 1,\n",
    "            eta_min = 1e-6,\n",
    "            last_epoch = -1\n",
    "        )\n",
    "        return sch\n",
    "    \n",
    "    \n",
    "    def fetch_optimizer(self):\n",
    "        ''' Adam optimization algo'''\n",
    "        opt = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr = 1e-4\n",
    "        )\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-02T11:46:56.133067Z",
     "iopub.status.busy": "2021-10-02T11:46:56.132717Z",
     "iopub.status.idle": "2021-10-02T11:47:01.147743Z",
     "shell.execute_reply": "2021-10-02T11:47:01.14697Z",
     "shell.execute_reply.started": "2021-10-02T11:46:56.133031Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction(KFold=10):\n",
    "    dense_features = [\n",
    "            'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "            'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
    "        ]\n",
    "    \n",
    "    super_final_predictions = []\n",
    "    for fold in range(KFold):   \n",
    "        print(f'fold = {fold}')\n",
    "        model = Model(model_name= 'tf_efficientnet_b0_ns')         \n",
    "        # model.load(f\"/kaggle/working/model_cv{fold}.bin\", device=\"cuda\", weights_only=True) \n",
    "        model.load(f\"../input/model-cv01234/model_cv{fold}.bin\", device=\"cuda\", weights_only=True) \n",
    "\n",
    "        df_test = pd.read_csv(os.path.join(path_input, 'test.csv'))\n",
    "        test_img_paths = [os.path.join(path_input, f'test/{img}.jpg') for img in df_test[\"Id\"].values] \n",
    "        \n",
    "        test_dataset = Dataset(\n",
    "            image_paths=test_img_paths,\n",
    "            dense_features=df_test[dense_features].values,\n",
    "            targets=np.ones(len(test_img_paths)),\n",
    "            augmentations=test_aug\n",
    "        )\n",
    "        \n",
    "        test_predictions = model.predict(test_dataset, \n",
    "                                         batch_size=2*Args.batch_size, \n",
    "                                         n_jobs=-1)\n",
    "\n",
    "        final_test_predictions = []\n",
    "        for preds in tqdm(test_predictions):\n",
    "            final_test_predictions.extend(preds.ravel().tolist())\n",
    "        # End of for preds\n",
    "        super_final_predictions.append(final_test_predictions)\n",
    "    # End of for fold\n",
    "    super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\n",
    "    df_test[\"Pawpularity\"] = super_final_predictions\n",
    "    df_test = df_test[[\"Id\", \"Pawpularity\"]]\n",
    "    df_test.to_csv(\"submission.csv\", index=False)\n",
    "    return df_test\n",
    "\n",
    "# RUN PREDICTION\n",
    "df_test = prediction(KFold=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "```\n",
    "Score = 20.467\n",
    "Key points: \n",
    "    (1) XGBRegressor model\n",
    "    (2) optuna lib: to define an objective function to be minimized\n",
    "    (3) KFold cross validation\n",
    "    (4) Use ONLY useful_features = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "                                    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n",
    "        NOT use images in the image dataset\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T10:41:24.034995Z",
     "iopub.status.busy": "2021-10-08T10:41:24.034665Z",
     "iopub.status.idle": "2021-10-08T10:41:24.755652Z",
     "shell.execute_reply": "2021-10-08T10:41:24.754880Z",
     "shell.execute_reply.started": "2021-10-08T10:41:24.034913Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from functools import partial\n",
    "import optuna\n",
    "import random\n",
    "\n",
    "path_input = r'../input/petfinder-pawpularity-score/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using optuna: to define an objective function to be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T10:41:24.757708Z",
     "iopub.status.busy": "2021-10-08T10:41:24.757329Z",
     "iopub.status.idle": "2021-10-08T10:41:24.770530Z",
     "shell.execute_reply": "2021-10-08T10:41:24.769718Z",
     "shell.execute_reply.started": "2021-10-08T10:41:24.757670Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial, fold, df, useful_features):\n",
    "    '''\n",
    "    To performs trials of parameter optimization for XGBoost regression model.\n",
    "    :param trial: optuna trial object to generate hyperparameters\n",
    "    :param fold: a fold in kfold for train and validation dataset. \n",
    "                 Ex. kfold = 10 (having 0, 1, .., 9) and fold = 2\n",
    "                 -> data at fold = 2 for validation and others for training\n",
    "    :param df: a dataframe from train.csv with added kfold column.\n",
    "        Ex.\n",
    "        Id                              Subject Focus  Eyes Face  Near Action Accessory Group Collage Human Occlusion Info Blur Pawpularity  kfold\n",
    "        0007de18844b0dbbb5e1f607da0606e0   0            1     1    1     0    0      1         0      0      0     0         0     63         2\n",
    "        0009c66b9439883ba2750fb825e1d7db   0            1     1    0     0    0      0         0      0      0     0         0     4          9\n",
    "    :param useful_features: column names of df using for X_train and X_valid\n",
    "        Ex. useful_features = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "                               'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n",
    "    return: RMSE score\n",
    "    '''\n",
    "    # Get Parameters for XGBRegressor\n",
    "    # η (learning_rate): Boosting learning rate (xgb's \"eta\")\n",
    "    η = trial.suggest_float('η', 1e-4, 0.5, log=True)  #  1e-2, 0.25\n",
    "    \n",
    "    # λ (reg_lambda): L2 regularization term on weights\n",
    "    λ = trial.suggest_loguniform('λ', 1e-9, 1000.0)  # 1e-8, 100.0\n",
    "    \n",
    "    # α (reg_alpha): L1 regularization term on weights\n",
    "    α = trial.suggest_loguniform('α', 1e-9, 1000.0)  # 1e-8, 100.0\n",
    "    \n",
    "    # subsample: Subsample ratio of the training instance.\n",
    "    subsample = trial.suggest_float('subsample', 0.01, 1.0) # 0.1, 1.0\n",
    "    \n",
    "    # colsample_bytree: Subsample ratio of columns when constructing each tree.\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.01, 1.0) # 0.1, 1.0\n",
    "    \n",
    "    # max_depth: Maximum tree depth for base learners.\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 20)  # 1, 7\n",
    "    \n",
    "    # Get train and validation dataset\n",
    "    X_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    X_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    Y_train = X_train.Pawpularity\n",
    "    Y_valid = X_valid.Pawpularity\n",
    "    \n",
    "    X_train = X_train[useful_features]\n",
    "    X_valid = X_valid[useful_features]\n",
    "    \n",
    "    # Model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=50000, # Number of gradient boosted trees. Equivalent to number of boosting rounds.  10000\n",
    "        tree_method='gpu_hist', # Default: auto (XGBoost will choose the most conservative option available)\n",
    "        random_state=42, # Random number seed\n",
    "        gpu_id=0, # Device ordinal. 0 or 1, ..\n",
    "        predictor='gpu_predictor',\n",
    "        \n",
    "        # BELOW: searching for parameter optimization\n",
    "        learning_rate=η,\n",
    "        reg_lambda=λ,\n",
    "        reg_alpha=α,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        max_depth=max_depth        \n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, Y_train,\n",
    "              early_stopping_rounds=1000,  # 300\n",
    "              eval_set=[(X_valid, Y_valid)],\n",
    "              verbose=1000)\n",
    "    \n",
    "    # Prediction\n",
    "    Y_valid_pred = model.predict(X_valid)\n",
    "    \n",
    "    # rmse\n",
    "    rmse = mean_squared_error(Y_valid, Y_valid_pred, squared=False)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T10:41:24.774167Z",
     "iopub.status.busy": "2021-10-08T10:41:24.773911Z",
     "iopub.status.idle": "2021-10-08T10:43:32.911792Z",
     "shell.execute_reply": "2021-10-08T10:43:32.910677Z",
     "shell.execute_reply.started": "2021-10-08T10:41:24.774142Z"
    }
   },
   "outputs": [],
   "source": [
    "KFold = 10  \n",
    "df = pd.read_csv(os.path.join(path_input, 'train.csv')).reset_index(drop=True)\n",
    "\n",
    "# Create kfold column to split train and validation dataset\n",
    "df[\"kfold\"] = [random.randint(0, KFold) for i in range(len(df))]\n",
    "\n",
    "# Get df_test\n",
    "df_test = pd.read_csv(os.path.join(path_input,'test.csv')).reset_index(drop=True)\n",
    "sample_submission = pd.read_csv(os.path.join(path_input,'sample_submission.csv'))\n",
    "\n",
    "# Get feature columns (for X)\n",
    "useful_features = [\n",
    "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
    "]\n",
    "\n",
    "opt_fun = partial(\n",
    "    objective,\n",
    "    fold=0,\n",
    "    df=df,\n",
    "    useful_features=useful_features,\n",
    ")\n",
    "\n",
    "# At optuna lib: Create a new study.\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Invoke optimization of the objective function.\n",
    "study.optimize(opt_fun, n_trials=10)  # 200, 1000\n",
    "print(f'study.best_params \\n {study.best_params}')\n",
    "print(f'study.best_value \\n {study.best_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T10:43:32.913988Z",
     "iopub.status.busy": "2021-10-08T10:43:32.913743Z",
     "iopub.status.idle": "2021-10-08T10:43:32.923820Z",
     "shell.execute_reply": "2021-10-08T10:43:32.923001Z",
     "shell.execute_reply.started": "2021-10-08T10:43:32.913953Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_predictions(params, fold, df, df_test, useful_features): \n",
    "    '''\n",
    "    Very similar to def objective(trial, fold, df, useful_features)\n",
    "    Note here: change trial to params\n",
    "    \n",
    "    :param params: the study.best_params\n",
    "    :param fold: a fold in kfold for train and validation dataset. \n",
    "                 Ex. kfold = 10 (having 0, 1, .., 9) and fold = 2\n",
    "                 -> data at fold = 2 for validation and others for training\n",
    "    :param df: a dataframe from train.csv with added kfold column.\n",
    "        Ex.\n",
    "        Id                              Subject Focus  Eyes Face  Near Action Accessory Group Collage Human Occlusion Info Blur Pawpularity  kfold\n",
    "        0007de18844b0dbbb5e1f607da0606e0   0            1     1    1     0    0      1         0      0      0     0         0     63         2\n",
    "        0009c66b9439883ba2750fb825e1d7db   0            1     1    0     0    0      0         0      0      0     0         0     4          9\n",
    "    \n",
    "    :param df_test: from Kaggle\n",
    "    \n",
    "    :param useful_features: column names of df using for X_train and X_valid\n",
    "        Ex. useful_features = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "                               'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n",
    "    return: RMSE score\n",
    "    '''\n",
    "    \n",
    "    # Get train, validation and test datasets\n",
    "    X_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    X_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    X_test = df_test.copy(deep=True) # deep copy of df_test\n",
    "    \n",
    "    Y_train = X_train.Pawpularity\n",
    "    Y_valid = X_valid.Pawpularity  # Note: No Y_test here\n",
    "    \n",
    "    X_train = X_train[useful_features]\n",
    "    X_valid = X_valid[useful_features]\n",
    "    X_test = X_test[useful_features]\n",
    "    \n",
    "    # Model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=50000, # Number of gradient boosted trees. Equivalent to number of boosting rounds.   10000\n",
    "        tree_method='gpu_hist', # Default: auto (XGBoost will choose the most conservative option available)\n",
    "        random_state=42, # Random number seed\n",
    "        gpu_id=0, # Device ordinal.\n",
    "        predictor='gpu_predictor',          \n",
    "        # Un-pack params        \n",
    "        **params,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, Y_train,\n",
    "              early_stopping_rounds=1000, # 300\n",
    "              eval_set=[(X_valid, Y_valid)],\n",
    "              verbose=1000) # 1000\n",
    "    \n",
    "    # Prediction\n",
    "    Y_valid_pred = model.predict(X_valid)\n",
    "    Y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # rmse: here for printing\n",
    "    rmse = mean_squared_error(Y_valid, Y_valid_pred, squared=False)\n",
    "    print(f'rmse = {rmse}')\n",
    "    \n",
    "    return Y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T10:43:32.925482Z",
     "iopub.status.busy": "2021-10-08T10:43:32.925213Z",
     "iopub.status.idle": "2021-10-08T10:43:55.697118Z",
     "shell.execute_reply": "2021-10-08T10:43:55.696016Z",
     "shell.execute_reply.started": "2021-10-08T10:43:32.925451Z"
    }
   },
   "outputs": [],
   "source": [
    "final_predictions = []\n",
    "for fold in range(KFold):\n",
    "    Y_test_pred = generate_predictions(params=study.best_params, \n",
    "                                       fold=fold, \n",
    "                                       df=df, \n",
    "                                       df_test=df_test, \n",
    "                                       useful_features=useful_features)    \n",
    "    final_predictions.append(Y_test_pred)\n",
    "# End of for\n",
    "final_predictions = np.mean(np.column_stack(final_predictions), axis=1)\n",
    "sample_submission.Pawpularity = final_predictions\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T10:43:55.698785Z",
     "iopub.status.busy": "2021-10-08T10:43:55.698513Z",
     "iopub.status.idle": "2021-10-08T10:43:56.430899Z",
     "shell.execute_reply": "2021-10-08T10:43:56.430192Z",
     "shell.execute_reply.started": "2021-10-08T10:43:55.698749Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/\n",
    "df = pd.read_csv('//kaggle/working/submission.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
